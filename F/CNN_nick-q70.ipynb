{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Pi0 Classification\n",
    "\n",
    "In this notebook, we train 10 layers deep CNN for particle type classification ($e^-$, $\\mu^-$, and $\\gamma$) using the workshop dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import torch, time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a network\n",
    "Let's define our network. The design below consists of 8 convolution layers + 3 fully-connected layers (10 learnable layers). Here is a summary of the graph operations.\n",
    "* Feature extractor:\n",
    "    1. Input shape: (N,88,168,2) ... N samples of 88x168 2D images with 2 channels\n",
    "    2. 2x Convolution layer + BatchNorm + ReLU, 32 filters, kernel size 3x3, stride 1 (default)\n",
    "    3. 2D max-pooling, kernel size 2, stride 2\n",
    "    4. 2x Convolution layer + BatchNorm + ReLU, 32 filters, kernel size 3x3, stride 1 (default)\n",
    "    5. 2D max-pooling, kernel size 2, stride 2\n",
    "    6. 2x Convolution layer + BatchNorm + ReLU, 64 filters, kernel size 3x3, stride 1 (default)\n",
    "    7. 2D max-pooling, kernel size 2, stride 2\n",
    "    8. 2x Convolution layer + BatchNorm + ReLU, 128 filters, kernel size 3x3, stride 1 (default)\n",
    "* Flattening\n",
    "    9. 2D average-pooling, kernel size = 2D image spatial dimension at this point (results in length 128 1D array)\n",
    "* Classifier:\n",
    "    10. Fully-connected layer + BatchNorm + ReLU, 128 filters\n",
    "    11. Fully-connected layer + BatchNorm + ReLU, 128 filters\n",
    "    12. Fully-connected layer, M filters where M = number of classification categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        # feature extractor CNN\n",
    "        self._feature = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(2,32,3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32,32,3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(32,32,3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32,32,3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(32,64,3), torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,64,3), torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(64,128,3), torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,3), torch.nn.BatchNorm2d(128), torch.nn.ReLU())\n",
    "        self._classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128,128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self._feature(x)\n",
    "        net = torch.nn.AvgPool2d(net.size()[2:])(net)\n",
    "        return self._classifier(net.view(-1,128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a train loop\n",
    "For convenience, define a _BLOB_ class to keep objects together. To a BLOB instance, we attach LeNet, our loss function (`nn.CrossEntropyLoss`), and Adam optimizer algorithm. For analysis purpose, we also include `nn.Softmax`. Finally, we attach data and label place holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLOB:\n",
    "    pass\n",
    "blob=BLOB()\n",
    "blob.net       = CNN(2).cuda() # construct Lenet for 2 class classification, use GPU\n",
    "blob.criterion = torch.nn.CrossEntropyLoss() # use softmax loss to define an error\n",
    "blob.optimizer = torch.optim.Adam(blob.net.parameters()) # use Adam optimizer algorithm\n",
    "blob.softmax   = torch.nn.Softmax(dim=1) # not for training, but softmax score for each class\n",
    "blob.data      = None # data for training/analysis\n",
    "blob.label     = None # label for training/analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define 2 functions to be called in the training loop: forward and backward. These functions implement the evaluation of the results, error (loss) definition, and propagation of errors (gradients) back to update the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(blob,train=True):\n",
    "    \"\"\"\n",
    "       Args: blob should have attributes, net, criterion, softmax, data, label\n",
    "       Returns: a dictionary of predicted labels, softmax, loss, and accuracy\n",
    "    \"\"\"\n",
    "    with torch.set_grad_enabled(train):\n",
    "        # Prediction\n",
    "        data = torch.as_tensor(blob.data).cuda()#[torch.as_tensor(d).cuda() for d in blob.data]\n",
    "        data = data.permute(0,3,1,2)\n",
    "        prediction = blob.net(data)\n",
    "        # Training\n",
    "        loss,acc=-1,-1\n",
    "        if blob.label is not None:\n",
    "            label = torch.as_tensor(blob.label).type(torch.LongTensor).cuda()#[torch.as_tensor(l).cuda() for l in blob.label]\n",
    "            label.requires_grad = False\n",
    "            loss = blob.criterion(prediction,label)\n",
    "        blob.loss = loss\n",
    "        \n",
    "        softmax    = blob.softmax(prediction).cpu().detach().numpy()\n",
    "        prediction = torch.argmax(prediction,dim=-1)\n",
    "        accuracy   = (prediction == label).sum().item() / float(prediction.nelement())        \n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        return {'prediction' : prediction,\n",
    "                'softmax'    : softmax,\n",
    "                'loss'       : loss.cpu().detach().item(),\n",
    "                'accuracy'   : accuracy}\n",
    "\n",
    "def backward(blob):\n",
    "    blob.optimizer.zero_grad()  # Reset gradients accumulation\n",
    "    blob.loss.backward()\n",
    "    blob.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a train loop \n",
    "Let's prepare the data loaders for both train and test datasets. We use the latter to check if the network suffers from overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146  events selected\n",
      "7181  events selected\n",
      "11203  events selected\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "from iotools import loader_factory\n",
    "DATA_DIRS=['/data/hkml_data/IWCDgrid/varyAll/e-','/data/hkml_data/IWCDgrid/varyAll/pi0']\n",
    "# for train\n",
    "def transform(result):\n",
    "    if result[1] == 3:\n",
    "        result[1] = 0\n",
    "#    result[1] -= 1\n",
    "    return result\n",
    "    \n",
    "#train_loader=loader_factory('H5Dataset', batch_size=64, shuffle=True, num_workers=4, data_dirs=DATA_DIRS, flavour='100k.h5', start_fraction=0.0, use_fraction=0.5, transform=transform, energy_min=0, energy_max=500)\n",
    "# for validation\n",
    "train_loaders = [loader_factory('H5Dataset', batch_size=64, shuffle=True, num_workers=4, data_dirs=DATA_DIRS, flavour='100k_q70.h5', start_fraction=0.0, use_fraction=0.5, transform=transform, energy_min=0, energy_max=e) for e in range(100,1001,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loader=loader_factory('H5Dataset', batch_size=200, shuffle=True, num_workers=2, data_dirs=DATA_DIRS, flavour='100k.h5', start_fraction=0.5, use_fraction=0.5, transform=transform, energy_min=0, energy_max=500)\n",
    "#test_loader2=loader_factory('H5Dataset', batch_size=200, shuffle=True, num_workers=2, data_dirs=DATA_DIRS, flavour='100k.h5', start_fraction=0.5, use_fraction=0.5, transform=transform, energy_min=500, energy_max=1000)\n",
    "test_loaders = [loader_factory('H5Dataset', batch_size=200, shuffle=True, num_workers=2, data_dirs=DATA_DIRS, flavour='100k_q70.h5', start_fraction=0.5, use_fraction=0.5, transform=transform, energy_min=0, energy_max=e) for e in range(100,1001,100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders2 = [loader_factory('H5Dataset', batch_size=200, shuffle=True, num_workers=2, data_dirs=DATA_DIRS, flavour='100k_q70.h5', start_fraction=0.5, use_fraction=0.5, transform=transform, energy_min=e-100, energy_max=e) for e in range(100,1001,100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also import `CSVData` from our utility module, which lets us write train log (accuracy, loss, etc.) in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 0) progress bar and 1) data recording utility (into csv file)\n",
    "from utils import progress_bar, CSVData\n",
    "blob.train_log, blob.test_log = CSVData('log_train.csv'), CSVData('log_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the training! Let's create a dataloader, write a loop to  call forward and backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define train period. \"epoch\" = N image consumption where N is the total number of train samples.\n",
    "print(\"15 epochs\")\n",
    "TRAIN_EPOCH=15.0\n",
    "# Set the network to training mode\n",
    "blob.net.train()\n",
    "epoch=0.\n",
    "iteration=0\n",
    "# Start training\n",
    "for iepoch in range(15):\n",
    "#while epoch < TRAIN_EPOCH:\n",
    "    iloader = min(iepoch,9)\n",
    "    print('Epoch',int(epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    # Create a progress bar for this epoch\n",
    "    from utils import progress_bar\n",
    "    progress = display(progress_bar(0,len(train_loaders[iloader])),display_id=True)\n",
    "    # Loop over data samples and into the network forward function\n",
    "    for i,data in enumerate(train_loaders[iloader]):\n",
    "        # Data and label\n",
    "        blob.data,blob.label = data[0:2]\n",
    "        # Call forward: make a prediction & measure the average error\n",
    "        res = forward(blob,True)\n",
    "        # Call backward: backpropagate error and update weights\n",
    "        backward(blob)\n",
    "        # Epoch update\n",
    "        epoch += 1./len(train_loaders[iloader])\n",
    "        iteration += 1\n",
    "        \n",
    "        #\n",
    "        # Log/Report\n",
    "        #\n",
    "        # Record the current performance on train set\n",
    "        blob.train_log.record(['iteration','epoch','accuracy','loss'],[iteration,epoch,res['accuracy'],res['loss']])\n",
    "        blob.train_log.write()\n",
    "        # once in a while, report\n",
    "        if i==0 or (i+1)%10 == 0:\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (iteration,epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(train_loaders[iloader]),message))\n",
    "        # more rarely, run validation\n",
    "        if (i+1)%100 == 0:\n",
    "            with torch.no_grad():\n",
    "                blob.net.eval()\n",
    "                test_data = next(iter(test_loaders[iloader]))\n",
    "                blob.data,blob.label = test_data[0:2]\n",
    "                res = forward(blob,False)\n",
    "                blob.test_log.record(['iteration','epoch','accuracy','loss'],[iteration,epoch,res['accuracy'],res['loss']])\n",
    "                blob.test_log.write()\n",
    "            blob.net.train()\n",
    "        if epoch >= TRAIN_EPOCH:\n",
    "            break\n",
    "    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (iteration,epoch,res['loss'],res['accuracy'])\n",
    "    progress.update(progress_bar((i+1),len(train_loaders[iloader]),message))\n",
    "\n",
    "blob.test_log.close()\n",
    "blob.train_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the training process\n",
    "Let's plot the train log for both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_log = pd.read_csv(blob.train_log.name)\n",
    "test_log  = pd.read_csv(blob.test_log.name)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Train loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(test_log.epoch, test_log.loss, marker='o', markersize=12, linestyle='', label='Test loss', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Train accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(test_log.epoch, test_log.accuracy, marker='o', markersize=12, linestyle='', label='Test accuracy', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the loss is coming down while the accuracy is increasing. These two should be anti-correlated, so this is expected. We also see the network performance on the test dataset (circles) follow those of train dataset (lines). This means there is no apparent overtraining.\n",
    "\n",
    "**Question: is the network still learning?**\n",
    "Both the loss and accuracy curve have large fluctuations and it is somewhat hard to see if the values are still changing. Let's plot the moving average of the loss and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "epoch    = moving_average(np.array(train_log.epoch),40)\n",
    "accuracy = moving_average(np.array(train_log.accuracy),40)\n",
    "loss     = moving_average(np.array(train_log.loss),40)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "line11 = ax1.plot(train_log.epoch, train_log.loss, linewidth=2, label='Loss', color='b', alpha=0.3)\n",
    "line12 = ax1.plot(epoch, loss, label='Loss (averaged)', color='blue')\n",
    "ax1.set_xlabel('Epoch',fontweight='bold',fontsize=24,color='black')\n",
    "ax1.tick_params('x',colors='black',labelsize=18)\n",
    "ax1.set_ylabel('Loss', fontsize=24, fontweight='bold',color='b')\n",
    "ax1.tick_params('y',colors='b',labelsize=18)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line21 = ax2.plot(train_log.epoch, train_log.accuracy, linewidth=2, label='Accuracy', color='r', alpha=0.3)\n",
    "line22 = ax2.plot(epoch, accuracy, label='Accuracy (averaged)', color='red')\n",
    "\n",
    "ax2.set_ylabel('Accuracy', fontsize=24, fontweight='bold',color='r')\n",
    "ax2.tick_params('y',colors='r',labelsize=18)\n",
    "ax2.set_ylim(0.,1.0)\n",
    "\n",
    "# added these four lines\n",
    "lines  = line11 + line12 + line21 + line22\n",
    "labels = [l.get_label() for l in lines]\n",
    "leg    = ax1.legend(lines, labels, fontsize=16, loc=5)\n",
    "leg_frame = leg.get_frame()\n",
    "leg_frame.set_facecolor('white')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tick lines now represent the moving average (all data points are from the train log). It appears the network is still learning. We can train for a longer period to achieve a better accuracy.\n",
    "\n",
    "## Performance Analysis\n",
    "Beyond looking at the performance of the network, we can analyze how the network is performing for each classification target. Let's first obtain a high-statistics analysis output by running the network on all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(blob,data_loader):\n",
    "    label,prediction,accuracy=[],[],[]\n",
    "    # set the network to test (non-train) mode\n",
    "    blob.net.eval()\n",
    "    # create the result holder\n",
    "    index,label,prediction = [],[],[]\n",
    "    for i,data in enumerate(data_loader):\n",
    "        blob.data, blob.label = data[0:2]\n",
    "        res = forward(blob,True)\n",
    "        accuracy.append(res['accuracy'])\n",
    "        prediction.append(res['prediction'])\n",
    "        label.append(blob.label)\n",
    "        #if i==2: break\n",
    "    # report accuracy\n",
    "    accuracy   = np.array(accuracy,dtype=np.float32)\n",
    "    label      = np.hstack(label)\n",
    "    prediction = np.hstack(prediction)\n",
    "    \n",
    "    return accuracy, label, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the inference using this function on the test sample, and look at the error matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import plot_confusion_matrix\n",
    "accuracies = []\n",
    "mins = []\n",
    "maxs = []\n",
    "for i in range(10):\n",
    "    emin = i*100\n",
    "    emax = (i+1)*100\n",
    "    print(emin,\"to\",emax,\"MeV\")\n",
    "    accuracy,label,prediction = inference(blob,test_loaders2[i])\n",
    "    print('Accuracy mean',accuracy.mean(),'std',accuracy.std())\n",
    "    plot_confusion_matrix(label,prediction,['pi0','electron'])\n",
    "    accuracies += [accuracy.mean()]\n",
    "    mins += [emin]\n",
    "    maxs += [emax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick',labelsize=20)\n",
    "plt.rc('ytick',labelsize=20)\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(maxs,accuracies, 'bo', markersize=15)\n",
    "plt.xlabel('Max energy [MeV]', fontsize=25)\n",
    "plt.ylabel('Accuracy', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import plot_confusion_matrix\n",
    "accuracies2 = []\n",
    "mins2 = []\n",
    "maxs2 = []\n",
    "for i in range(10):\n",
    "    emin = 0\n",
    "    emax = (i+1)*100\n",
    "    print(emin,\"to\",emax,\"MeV\")\n",
    "    accuracy,label,prediction = inference(blob,test_loaders[i])\n",
    "    print('Accuracy mean',accuracy.mean(),'std',accuracy.std())\n",
    "    plot_confusion_matrix(label,prediction,['pi0','electron'])\n",
    "    accuracies2 += [accuracy.mean()]\n",
    "    mins2 += [emin]\n",
    "    maxs2 += [emax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick',labelsize=20)\n",
    "plt.rc('ytick',labelsize=20)\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(maxs2,accuracies2, 'bo', markersize=15)\n",
    "plt.xlabel('Max energy [MeV]', fontsize=25)\n",
    "plt.ylabel('Accuracy', fontsize=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
